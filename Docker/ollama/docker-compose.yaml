# Docker Compose configuration for Ollama - LLM inference server

services:
    ollama:
        image: ollama/ollama
        container_name: ollama
        hostname: ollama
        networks:
          - ollama-network
        ports:
            - '11434:11434'
        volumes:
            - '/mnt/user/appdata/ollama:/root/.ollama'
        deploy:
            resources:
                reservations:
                    devices:
                        - driver: nvidia
                          count: all
                          capabilities: [ gpu ]

networks:
  # Ollama-Network - Docker service
  ollama-network:
    name: ollama-network


    