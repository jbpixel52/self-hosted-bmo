services:
  searxng:
    container_name: searxng
    image: docker.io/searxng/searxng:latest
    restart: unless-stopped
    hostname: searxng
    networks:
      - search
    ports:
      - "8089:8080"
    volumes:
      - /mnt/user/appdata/searxng/:/etc/searxng:rw
    environment:
      - SEARXNG_BASE_URL=https://${SEARXNG_HOSTNAME:-localhost}/
      - UWSGI_WORKERS=${SEARXNG_UWSGI_WORKERS:-4}
      - UWSGI_THREADS=${SEARXNG_UWSGI_THREADS:-4}
    cap_drop:
      - ALL
    cap_add:
      - CHOWN
      - SETGID
      - SETUID
    logging:
      driver: "json-file"
      options:
        max-size: "1m"
        max-file: "1"

  ollama:
        image: ollama/ollama
        container_name: ollama
        hostname: ollama
        networks:
          - search
        ports:
            - '11434:11434'
        volumes:
            - '/mnt/user/appdata/ollama:/root/.ollama'
        deploy:
            resources:
                reservations:
                    devices:
                        - driver: nvidia
                          count: all
                          capabilities: [ gpu ]
  # perplexica-backend:
  #   image: rqi14/perplexica-backend:slim
  #   depends_on:
  #     - searxng
  #   ports:
  #     - 3001:3001
  #   volumes:
  #     - /mnt/user/appdata/perplexica/backend-dbstore:/home/perplexica/data
  #   extra_hosts:
  #     - 'host.docker.internal:host-gateway'
  #   networks:
  #     - search
  #   restart: unless-stopped

  # perplexica-frontend:
  #   build:
  #     context: .
  #     dockerfile: app.dockerfile
  #     args:
  #       - NEXT_PUBLIC_API_URL=http://127.0.0.1:3001/api
  #       - NEXT_PUBLIC_WS_URL=ws://127.0.0.1:3001
  #   depends_on:
  #     - perplexica-backend
  #   ports:
  #     - 3000:3000
  #   networks:
  #     - perplexica-network
  #   restart: unless-stopped
networks:
  search:

volumes:
  valkey-data2:
